{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n",
    "!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-26T16:30:07.271901Z",
     "iopub.status.busy": "2023-10-26T16:30:07.271626Z",
     "iopub.status.idle": "2023-10-26T16:30:11.873245Z",
     "shell.execute_reply": "2023-10-26T16:30:11.872424Z",
     "shell.execute_reply.started": "2023-10-26T16:30:07.271878Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "MAX_INPUT = 256\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WIKI Context Using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:30:33.873711Z",
     "iopub.status.busy": "2023-10-26T16:30:33.872857Z",
     "iopub.status.idle": "2023-10-26T16:30:33.87823Z",
     "shell.execute_reply": "2023-10-26T16:30:33.877222Z",
     "shell.execute_reply.started": "2023-10-26T16:30:33.873681Z"
    }
   },
   "outputs": [],
   "source": [
    "use_long_context = True\n",
    "device = 'cuda'\n",
    "max_length = 384\n",
    "batch_size = 16\n",
    "if use_long_context:\n",
    "    num_top_pages = 5\n",
    "else:\n",
    "    num_top_pages = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = '/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(sentence_transformer_model, device=device)\n",
    "model.max_seq_length = max_length\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings = model.encode(test_df['prompt'].values, batch_size=batch_size, device=device, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_score, search_index = sentence_index.search(prompt_embeddings, num_top_pages)\n",
    "del sentence_index\n",
    "del prompt_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_score[0], search_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_source_df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\", columns=['id', 'file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_source_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_wiki_file_dfs = []\n",
    "for i in range(len(search_index)):\n",
    "    cur_wiki_source_df = wiki_source_df.loc[search_index[i]].copy()\n",
    "    cur_wiki_source_df['orig_prompt_id'] = i\n",
    "    prompt_to_wiki_file_dfs.append(cur_wiki_source_df)\n",
    "prompt_to_wiki_final_df = pd.concat(prompt_to_wiki_file_dfs).reset_index(drop=True)\n",
    "del wiki_source_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_wiki_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_base_path = \"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_test_df = pd.read_parquet(f\"{wiki_base_path}/a.parquet\")\n",
    "print(wiki_test_df.dtypes)\n",
    "wiki_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wiki_test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text_dfs = []\n",
    "unique_wiki_files = prompt_to_wiki_final_df['file'].unique()\n",
    "for unique_wiki_file in unique_wiki_files:\n",
    "    wiki_ids = [str(x) for x in prompt_to_wiki_final_df.loc[prompt_to_wiki_final_df['file']==unique_wiki_file]['id'].values]\n",
    "    cur_wiki_text_df = pd.read_parquet(f\"{wiki_base_path}/{unique_wiki_file}\", columns=['id', 'text'])\n",
    "    final_wiki_text_df = cur_wiki_text_df.loc[cur_wiki_text_df['id'].isin(wiki_ids)].copy()\n",
    "    wiki_text_dfs.append(final_wiki_text_df)\n",
    "    del cur_wiki_text_df\n",
    "    gc.collect()\n",
    "wiki_text_df_final = pd.concat(wiki_text_dfs).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text_df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text_df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 3,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the \n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = 3,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1]-o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_wiki_text_data = process_documents(wiki_text_df_final['text'].values, wiki_text_df_final['id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_wiki_text_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_wiki_text_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "                                    batch_size=batch_size,\n",
    "                                    device=device,\n",
    "                                    show_progress_bar=True,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)#.half()\n",
    "wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prompt_answer_text'] = test_df.apply(lambda x: x['prompt'] + \" \" + \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answer_embeddings = model.encode(test_df['prompt_answer_text'].values, batch_size=batch_size, device=device, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_answer_embeddings = prompt_answer_embeddings.detach().cpu().numpy()\n",
    "prompt_answer_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_wiki_text_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_long_context:\n",
    "    num_context_sentences = 20\n",
    "else:\n",
    "    num_context_sentences = 5\n",
    "context_texts = []\n",
    "\n",
    "for prompt_id in test_df.index.tolist():\n",
    "    context = ''\n",
    "    context_sent_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(prompt_to_wiki_final_df[prompt_to_wiki_final_df['orig_prompt_id']==prompt_id]['id'].values)].index.values\n",
    "    context_sent_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "    context_sent_index.add(wiki_data_embeddings[context_sent_indices])\n",
    "    _, x = context_sent_index.search(prompt_answer_embeddings, num_context_sentences)\n",
    "    for context_sent_idx in x[prompt_id]:\n",
    "        context += processed_wiki_text_data.loc[context_sent_indices, 'text'].iloc[context_sent_idx] + \" \"\n",
    "    context_texts.append(context.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['context'] = context_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_with_context = test_df[['prompt', 'context', 'A', 'B', 'C', 'D', 'E']]\n",
    "test_df_with_context.to_csv('./test_df_with_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./test_df_with_context.csv').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Inference on Question Answering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:20.767937Z",
     "iopub.status.busy": "2023-10-26T16:31:20.767112Z",
     "iopub.status.idle": "2023-10-26T16:31:20.771965Z",
     "shell.execute_reply": "2023-10-26T16:31:20.770996Z",
     "shell.execute_reply.started": "2023-10-26T16:31:20.767907Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:20.851586Z",
     "iopub.status.busy": "2023-10-26T16:31:20.851289Z",
     "iopub.status.idle": "2023-10-26T16:31:26.672359Z",
     "shell.execute_reply": "2023-10-26T16:31:26.67135Z",
     "shell.execute_reply.started": "2023-10-26T16:31:20.851562Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_long_context:\n",
    "    model_path = '/kaggle/input/fine-tuned-open-book-model/model_v2'\n",
    "else:\n",
    "    model_path = '/kaggle/input/llm-science-run-context-2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_path).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:26.674186Z",
     "iopub.status.busy": "2023-10-26T16:31:26.673861Z",
     "iopub.status.idle": "2023-10-26T16:31:26.714117Z",
     "shell.execute_reply": "2023-10-26T16:31:26.71318Z",
     "shell.execute_reply.started": "2023-10-26T16:31:26.67416Z"
    }
   },
   "outputs": [],
   "source": [
    "num_chars_in_context = 1750\n",
    "test_df_with_context = pd.read_csv('./test_df_with_context.csv')\n",
    "test_df_with_context['id'] = list(range(len(test_df_with_context)))\n",
    "test_df_with_context['prompt_with_context'] = test_df_with_context.apply(lambda x: x['context'][:num_chars_in_context] + ' #### ' + x['prompt'], axis=1)\n",
    "test_df_with_context['label'] = 0\n",
    "test_df_with_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:26.715689Z",
     "iopub.status.busy": "2023-10-26T16:31:26.715317Z",
     "iopub.status.idle": "2023-10-26T16:31:26.723194Z",
     "shell.execute_reply": "2023-10-26T16:31:26.72217Z",
     "shell.execute_reply.started": "2023-10-26T16:31:26.715656Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    options = 'ABCDE'\n",
    "    first_sentences = [examples[\"prompt_with_context\"]] * 5\n",
    "    second_sentences = [examples[options[option_ind]] for option_ind in range(len(options))]\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    tokenized_examples['label'] = examples['label']\n",
    "    return tokenized_examples\n",
    "\n",
    "def preprocess_long_context(example):\n",
    "    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
    "    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n",
    "                                  max_length=MAX_INPUT, add_special_tokens=False)\n",
    "    tokenized_example['label'] = example['label']\n",
    "    return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:26.725468Z",
     "iopub.status.busy": "2023-10-26T16:31:26.725157Z",
     "iopub.status.idle": "2023-10-26T16:31:26.734567Z",
     "shell.execute_reply": "2023-10-26T16:31:26.733698Z",
     "shell.execute_reply.started": "2023-10-26T16:31:26.725437Z"
    }
   },
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "cls_token_id, sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:26.736329Z",
     "iopub.status.busy": "2023-10-26T16:31:26.735789Z",
     "iopub.status.idle": "2023-10-26T16:31:26.746851Z",
     "shell.execute_reply": "2023-10-26T16:31:26.746041Z",
     "shell.execute_reply.started": "2023-10-26T16:31:26.736298Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:32.031048Z",
     "iopub.status.busy": "2023-10-26T16:31:32.030158Z",
     "iopub.status.idle": "2023-10-26T16:31:34.66027Z",
     "shell.execute_reply": "2023-10-26T16:31:34.659375Z",
     "shell.execute_reply.started": "2023-10-26T16:31:32.031003Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "if use_long_context:\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_df_with_context[['id', 'prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'label']]).map(preprocess_long_context, remove_columns=['id', 'prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'label'])\n",
    "else:\n",
    "    tokenized_test_dataset = Dataset.from_pandas(test_df_with_context[['id', 'prompt_with_context', 'A', 'B', 'C', 'D', 'E', 'label']]).map(preprocess_function, remove_columns=['id', 'prompt_with_context', 'A', 'B', 'C', 'D', 'E', 'label'])\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:31:39.670141Z",
     "iopub.status.busy": "2023-10-26T16:31:39.669031Z",
     "iopub.status.idle": "2023-10-26T16:32:16.664067Z",
     "shell.execute_reply": "2023-10-26T16:32:16.663127Z",
     "shell.execute_reply.started": "2023-10-26T16:31:39.670107Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "output_logits = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        output_logits.append(model(**batch).logits.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T16:32:16.666195Z",
     "iopub.status.busy": "2023-10-26T16:32:16.665879Z",
     "iopub.status.idle": "2023-10-26T16:32:16.680846Z",
     "shell.execute_reply": "2023-10-26T16:32:16.679959Z",
     "shell.execute_reply.started": "2023-10-26T16:32:16.666167Z"
    }
   },
   "outputs": [],
   "source": [
    "top_k_predictions = 3\n",
    "output_logits = np.argsort(-1 * np.stack(output_logits).squeeze(), axis=1)\n",
    "option_list = np.array('A B C D E'.split())\n",
    "final_predictions = [' '.join(option_list[output_logits[i, :top_k_predictions]]) for i in range(len(output_logits))]\n",
    "test_df_with_context['prediction'] = final_predictions\n",
    "test_df_with_context[['id', 'prediction']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
